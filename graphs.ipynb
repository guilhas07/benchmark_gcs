{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fe35d3-f7fc-4121-bcbc-0c3ece22c6ae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b786ff-792c-4f72-8b18-6a1ae4d04457",
   "metadata": {},
   "outputs": [],
   "source": [
    "throughput = {}\n",
    "pause_time = {}\n",
    "out = {}\n",
    "for gc in [\"G1\", \"Parallel\", \"Z\"]:\n",
    "    for runtime in [\"Graal\", \"HotSpot\"]:\n",
    "        files = list(Path(\"benchmark_stats/gc_stats\").glob(f\"{gc}_{runtime}*\"))\n",
    "        assert len(files) == 1\n",
    "        file = files[0]\n",
    "        with open(file) as f:\n",
    "            stats = json.loads(f.read())[\"stats\"]\n",
    "            assert isinstance(stats, list)\n",
    "            for stat in stats:\n",
    "                heap_size = stat[\"heap_size\"]\n",
    "                throughput.setdefault(runtime, {}).setdefault(heap_size, {})[\"avg_throughput\"] = stat[\"avg_throughput\"]\n",
    "                pause_time.setdefault(runtime, {}).setdefault(heap_size, {})[\"p90_avg_pause_time\"] = stat[\"p90_avg_pause_time\"]\n",
    "    hot_t, graal_t = throughput[\"HotSpot\"], throughput[\"Graal\"]\n",
    "    hot_p, graal_p = pause_time[\"HotSpot\"], pause_time[\"Graal\"]\n",
    "    for heap_size in hot_t:\n",
    "        t_hot = hot_t[heap_size][\"avg_throughput\"]\n",
    "        t_graal = graal_t[heap_size][\"avg_throughput\"]\n",
    "        p_hot = hot_p[heap_size][\"p90_avg_pause_time\"]\n",
    "        p_graal = graal_p[heap_size][\"p90_avg_pause_time\"]\n",
    "        out.setdefault(gc, {})[heap_size] = {\n",
    "            \"throughput_diff_ns\": t_graal - t_hot,\n",
    "            \"throughput_percentage\": (t_graal - t_hot)/t_hot * 100, \n",
    "            \"pause_time_diff_ns\": p_graal - p_hot, \n",
    "            \"pause_time_percentage\": (p_graal - p_hot)/p_hot  * 100, \n",
    "        }\n",
    "print(json.dumps(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af250670-61a6-423c-a2ae-1847fdac3339",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in Path(\"graphs\").glob(\"**/*.png\"):\n",
    "    file.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa6f121-c68d-47f1-bf99-82f661d3f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = Path(\"benchmark_matrix\")\n",
    "with open(dir / \"Graal_11_10_0_19.json\") as f:\n",
    "    graal = json.loads(f.read())[\"matrix\"]\n",
    "with open(dir / \"HotSpot_11_10_0_19.json\") as f:\n",
    "    hotspot = json.loads(f.read())[\"matrix\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644b2f0-c21f-4951-941f-ceab57f63da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix(key, d, file_title, y_label, y_limit, layout=None):\n",
    "    x_labels = list(d.keys())\n",
    "    x = np.arange(len(x_labels))\n",
    "    fig, ax = plt.subplots(figsize=(20, 10), layout=layout, dpi=300)\n",
    "    \n",
    "    gcs = sorted(graal[x_labels[0]])\n",
    "    #def plot_sub_graph(ax, key, d, gcs, title, y_label, y_limit):\n",
    "    width = 0.3\n",
    "    multiplier = 0\n",
    "    for gc in gcs:\n",
    "        values = [value[gc][key] for value in d.values()]\n",
    "        offset = width * multiplier\n",
    "        rects = ax.bar(x + offset, values, width, label=gc)\n",
    "        ax.bar_label(rects)\n",
    "        multiplier += 1\n",
    "        ## Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "        ax.set_ylabel(y_label)\n",
    "        ax.set_xlabel(\"Heap Sizes (MB)\")\n",
    "        #ax.set_title(title)\n",
    "        ax.set_xticks(x + width, x_labels)\n",
    "        ax.legend(ncols=3, fontsize=20)\n",
    "        ax.set_ylim(0, y_limit)\n",
    "        fig.savefig(f\"graphs/{file_title}.png\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4607f4-07cd-416b-9e40-82a8bfd4026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix(\"throughput\", graal,  \"throughput_graalvm\", \"Throughput\", 2, \"tight\")\n",
    "plot_matrix(\"throughput\", hotspot, \"throughput_hotspot\", \"Throughput\", 2, \"tight\")\n",
    "plot_matrix(\"pause_time\", graal, \"pause_time_graalvm\", \"Pause Time\", 2, \"tight\")\n",
    "plot_matrix(\"pause_time\", hotspot,  \"pause_time_hotspot\", \"Pause Time\",2.6, \"tight\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b1fa9d-7205-4617-9e65-9298570656be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_benchmarks(jdk, benchmark_group):\n",
    "    benchmarks = {}\n",
    "    files = [f for f in Path(\".\").glob(\"benchmark_stats/*.json\") if jdk in f.name and benchmark_group in f.name and \"error\" not in f.name]\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            benchmark = json.loads(f.read())\n",
    "            heap_size = benchmark[\"heap_size\"]\n",
    "            name = benchmark[\"benchmark_name\"]\n",
    "            gc = benchmark[\"garbage_collector\"]\n",
    "            throughput =  benchmark[\"throughput\"]\n",
    "            pause_time =  benchmark[\"p90_pause_time\"]\n",
    "            a = benchmarks.setdefault(heap_size, {}).setdefault(name, {}).setdefault(gc, {})\n",
    "            a[\"throughput\"] = throughput / 1e9\n",
    "            a[\"pause_time\"] = pause_time / 1e9\n",
    "            \n",
    "    # filter results not valid for all 3 GCs\n",
    "    #print(\"Before\", str(benchmarks)[:500])\n",
    "    for key, value in list(benchmarks.items()):\n",
    "        for k, v in list(value.items()):\n",
    "           #print(f\"Key={k} Value={v}\")\n",
    "           if len(v) != 3 or k == \"spring\": # filter spring\n",
    "               #print(f\"Deleting with keys: {key},{k}: {benchmarks[key][k]}\")\n",
    "               del benchmarks[key][k]\n",
    "               #print(f\"After delete with keys: {key},{k}: {benchmarks[key].get(k)}\")\n",
    "           #else:\n",
    "           #    benchmarks[key] = dict(sorted(value.items()))\n",
    "    for key in list(benchmarks):\n",
    "        #print(f\"Before: {benchmarks[key]}\")\n",
    "        benchmarks[key] = dict(sorted(benchmarks[key].items()))\n",
    "        #print(f\"After: {benchmarks[key]}\")\n",
    "\n",
    "    # assert everything is sorted\n",
    "    # heap_size : name: gc\n",
    "    for key in benchmarks:\n",
    "        assert list(benchmarks[key].keys()) == sorted(list(benchmarks[key].keys())), f\"Error: {list(benchmarks[key].keys())=} != {sorted(list(benchmarks[key].keys()))=}\"\n",
    "        #print(benchmarks[key].keys())\n",
    "        for name in benchmarks[key]:\n",
    "            assert len(benchmarks[key][name]) == 3, f\"Error, {benchmarks[key][name]}\"\n",
    "        #a = [f\"{benchmarks[key]} {v=} {len(v)=}\" for v in benchmarks[key].values()]\n",
    "        #print(a)\n",
    "    #assert len(benchmarks['256']['kafka']) == 3, \"hmm\"\n",
    "    #print(benchmarks['256']['kafka'])\n",
    "    #print(benchmarks.keys())\n",
    "    \n",
    "        \n",
    "    return benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4257bd-6a6a-4386-a662-d9caf265792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = load_benchmarks(\"HotSpot\", \"DaCapo\")\n",
    "#for gc in [\"Parallel\", \"G1\", \"Z\"]:\n",
    "#for key1 in a:\n",
    "#    assert len(a[key1]) == 3\n",
    "\n",
    "#print(a)\n",
    "#print(a['512'].keys())\n",
    "#print(a['512'].keys())\n",
    "#print(a['256']['tradesoap']['Parallel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0588ac9-e838-43ae-bd84-5a92e8883f03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_benchmarks(benchmarks, key, y_label, file_title, layout=None, y_limit=None):\n",
    "    x_labels = list(benchmarks.keys()) # This should be already sorted\n",
    "    print(f\"{x_labels=}\")\n",
    "    x = np.arange(len(x_labels))\n",
    "    fig, ax = plt.subplots(figsize=(20, 10), layout=layout, dpi=300)\n",
    "\n",
    "    gcs = sorted(benchmarks[x_labels[0]])\n",
    "    width = 0.3\n",
    "    multiplier = 0\n",
    "    for gc in gcs:\n",
    "        values = [value[gc][key] for value in benchmarks.values()]\n",
    "        offset = width * multiplier\n",
    "        rects = ax.bar(x + offset, values, width, label=gc)\n",
    "        multiplier += 1\n",
    "        ax.set_ylabel(y_label)\n",
    "        ax.set_xlabel(\"Benchmarks\")\n",
    "        ax.set_xticks(x + width, x_labels)\n",
    "        plt.xticks(rotation=45)\n",
    "        ax.legend(ncols=3)\n",
    "        ax.set_ylim(0, y_limit)\n",
    "        #ax.set_title(file_title)\n",
    "        #ax.bar_label(rects, label_type='center')\n",
    "        fig.savefig(f\"graphs/{file_title}.png\")\n",
    "        \n",
    "def find_increment(number):\n",
    "    n = str(number).split(\".\")\n",
    "    num = 1\n",
    "    if n[0] == \"0\":\n",
    "        num = 0.1\n",
    "        for i in n[1]:\n",
    "            num *= 0.1\n",
    "            if i != \"0\":\n",
    "                break\n",
    "    return num\n",
    "\n",
    "    \n",
    "#plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "for runtime, group in itertools.product([\"Graal\", \"HotSpot\"], [\"Renaissance\", \"DaCapo\"]):\n",
    "    benchmarks = load_benchmarks(runtime, group)\n",
    "    for heap_size in benchmarks:\n",
    "        plt.close(\"all\")\n",
    "        max_throughput = max([v2[\"throughput\"] for v1 in benchmarks[heap_size].values() for v2 in v1.values()])\n",
    "        max_latency = max([v2[\"pause_time\"] for v1 in benchmarks[heap_size].values() for v2 in v1.values()])\n",
    "        #print(\"Antes\", max_throughput, max_latency)\n",
    "        max_latency += find_increment(max_latency)\n",
    "        max_throughput += find_increment(max_throughput)\n",
    "        #print(\"Depois\", max_throughput, max_latency)\n",
    "        plot_benchmarks(benchmarks[heap_size], \"throughput\", \"Throughput (seconds)\", f\"{runtime}/{group}/{runtime}-{group}-{heap_size}-throughput\", layout='tight',y_limit=max_throughput)\n",
    "        plot_benchmarks(benchmarks[heap_size], \"pause_time\", \"Pause Time (seconds)\", f\"{runtime}/{group}/{runtime}-{group}-{heap_size}-pause_time\", layout='tight',y_limit=max_latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f03274e-5d4c-4e53-b296-27794fc8e695",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Graal\": {\"throughput\": {\"256\": {\"G1\": 100.956143022, \"Z\": 172.133560401, \"Parallel\": 100.721224507}, \"512\": {\"Z\": 111.566554946, \"Parallel\": 95.221145106, \"G1\": 97.730748951}, \"1024\": {\"Parallel\": 94.448982644, \"G1\": 96.986427753, \"Z\": 100.82124546}, \"2048\": {\"Parallel\": 92.472127809, \"G1\": 96.924042931, \"Z\": 96.371266886}, \"4096\": {\"Parallel\": 91.557172331, \"G1\": 96.931326247, \"Z\": 96.587391626}, \"8192\": {\"G1\": 96.032675242, \"Z\": 97.710785503, \"Parallel\": 91.880692109}}, \"pause_time\": {\"256\": {\"G1\": 0.011665659, \"Z\": 0.0020112408, \"Parallel\": 0.0084089566}, \"512\": {\"Z\": 0.0030161742, \"Parallel\": 0.008539155199999999, \"G1\": 0.01274278}, \"1024\": {\"Parallel\": 0.0086615358, \"G1\": 0.014064722699999999, \"Z\": 0.003198721}, \"2048\": {\"Parallel\": 0.0079225928, \"G1\": 0.014682675, \"Z\": 0.0023168136000000002}, \"4096\": {\"Parallel\": 0.0074958221, \"G1\": 0.020586179, \"Z\": 0.0021158837999999997}, \"8192\": {\"G1\": 0.025045479, \"Z\": 0.0021154329, \"Parallel\": 0.0064246653}}}, \"HotSpot\": {\"throughput\": {\"256\": {\"Parallel\": 100.378031, \"G1\": 50.916234983, \"Z\": 167.291581058}, \"512\": {\"Parallel\": 90.69549183, \"G1\": 92.10755095, \"Z\": 106.543211008}, \"1024\": {\"G1\": 90.883709011, \"Z\": 93.447628677, \"Parallel\": 93.135048055}, \"2048\": {\"Z\": 89.493152329, \"Parallel\": 89.347972496, \"G1\": 93.958533142}, \"4096\": {\"Z\": 91.155497932, \"Parallel\": 90.616044369, \"G1\": 92.423337998}, \"8192\": {\"G1\": 91.08686908, \"Parallel\": 88.301998429, \"Z\": 94.746560235}}, \"pause_time\": {\"256\": {\"Parallel\": 0.0085172406, \"G1\": 0.0017711867, \"Z\": 0.0026704054}, \"512\": {\"Parallel\": 0.008806797, \"G1\": 0.0128748283, \"Z\": 0.0035242493}, \"1024\": {\"G1\": 0.014467395800000001, \"Z\": 0.0031629936, \"Parallel\": 0.0088380044}, \"2048\": {\"Z\": 0.0021545822, \"Parallel\": 0.0096731241, \"G1\": 0.0228605038}, \"4096\": {\"Z\": 0.0026818152000000002, \"Parallel\": 0.009853158400000001, \"G1\": 0.024532876800000002}, \"8192\": {\"G1\": 0.028264463899999998, \"Parallel\": 0.0088761951, \"Z\": 0.003891957}}}}\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (1784715630.py, line 63)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[37], line 63\u001b[0;36m\u001b[0m\n\u001b[0;31m    return\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "def plot_benchmarks(benchmarks, y_label, file_title, layout=None, y_limit=None):\n",
    "    # benchmarks -> dict[heap_size, dict[gc, value]]\n",
    "    x_labels = list(benchmarks) # This should be already sorted\n",
    "    print(f\"{x_labels=}\") # x_lables = [heap_size]\n",
    "    x = np.arange(len(x_labels))\n",
    "    fig, ax = plt.subplots(figsize=(20, 10), layout=layout, dpi=300)\n",
    "\n",
    "    gcs = sorted(benchmarks[x_labels[0]]) # sorted gcs\n",
    "    width = 0.3\n",
    "    multiplier = 0\n",
    "    for gc in gcs:\n",
    "        values = [d[gc] for d in benchmarks.values()]\n",
    "        offset = width * multiplier\n",
    "        rects = ax.bar(x + offset, values, width, label=gc)\n",
    "        multiplier += 1\n",
    "        ax.set_ylabel(y_label)\n",
    "        ax.set_xlabel(\"Heap Sizes (MB)\")\n",
    "        ax.set_xticks(x + width, x_labels)\n",
    "        #plt.xticks(rotation=45)\n",
    "        ax.legend(ncols=3)\n",
    "        ax.set_ylim(0, y_limit)\n",
    "        #ax.set_title(file_title)\n",
    "        #ax.bar_label(rects, label_type='center')\n",
    "        fig.savefig(f\"graphs/{file_title}.png\")\n",
    "        \n",
    "def find_increment(number):\n",
    "    n = str(number).split(\".\")\n",
    "    num = 1\n",
    "    if n[0] == \"0\":\n",
    "        num = 0.1\n",
    "        for i in n[1]:\n",
    "            num *= 0.1\n",
    "            if i != \"0\":\n",
    "                break\n",
    "    return num\n",
    "\n",
    "    \n",
    "#plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "data = {}\n",
    "for runtime in [\"Graal\", \"HotSpot\"]:\n",
    "    benchmarks = list(Path(\"spring_stats\").glob(f\"*{runtime}*\"))\n",
    "    for benchmark in benchmarks:\n",
    "        with open(benchmark) as f:\n",
    "            d = json.loads(f.read())\n",
    "            assert runtime in d[\"jdk\"]\n",
    "            assert \"spring\" == d[\"benchmark_name\"]\n",
    "            assert d[\"error\"] is None\n",
    "            heap_size = d[\"heap_size\"]\n",
    "            gc = d[\"garbage_collector\"]\n",
    "            p90 = d[\"p90_pause_time\"]\n",
    "            throughput = d[\"throughput\"]\n",
    "            data.setdefault(runtime, {}).setdefault(\"throughput\", {}).setdefault(int(heap_size), {})[gc] = throughput / 1e9\n",
    "            data.setdefault(runtime, {}).setdefault(\"pause_time\", {}).setdefault(int(heap_size), {})[gc] = p90 / 1e9\n",
    "\n",
    "for key in list(data):\n",
    "    data[key][\"throughput\"] = dict(sorted(data[key][\"throughput\"].items()))\n",
    "    data[key][\"pause_time\"] = dict(sorted(data[key][\"pause_time\"].items()))\n",
    "    assert list(data[key][\"throughput\"]) == sorted(data[key][\"throughput\"])\n",
    "    assert list(data[key][\"pause_time\"]) == sorted(data[key][\"pause_time\"])\n",
    "\n",
    "print(json.dumps(data))\n",
    "return\n",
    "for runtime in data:\n",
    "    throughput_data = data[runtime][\"throughput\"]\n",
    "    pause_data = data[runtime][\"pause_time\"]\n",
    "    plt.close(\"all\")\n",
    "    max_throughput = max([v2 for gcs_dict in throughput_data.values() for v2 in gcs_dict.values()])\n",
    "    max_latency = max([v2 for gcs_dict in pause_data.values() for v2 in gcs_dict.values()])\n",
    "    #print(\"Antes\", max_throughput, max_latency)\n",
    "    max_latency += find_increment(max_latency)\n",
    "    max_throughput += find_increment(max_throughput)\n",
    "    #print(\"Depois\", max_throughput, max_latency)\n",
    "    plot_benchmarks(throughput_data, \"Throughput (seconds)\", f\"BestGC_plusplus/{runtime}/BestGC_{runtime}-throughput\", layout='tight',y_limit=max_throughput)\n",
    "    plot_benchmarks(pause_data, \"Pause Time (seconds)\", f\"BestGC_plusplus/{runtime}/BestGC_{runtime}-pause_time\", layout='tight',y_limit=max_latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68166d36-0a0b-43ea-b332-2f01fcc2c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_only_success():\n",
    "    benchmarks = {}\n",
    "    for file in Path(\"benchmark_stats/\").glob(\"Renaissance*256*Graal*.json\"):\n",
    "        #print(file)\n",
    "        with open(file) as f: \n",
    "            benchmark = json.loads(f.read())\n",
    "        key = \"error\" if \"error\" in f\"{file}\" else \"success\"\n",
    "        assert benchmark[\"benchmark_name\"] is not None\n",
    "        benchmarks.setdefault(benchmark[\"benchmark_name\"], {}).setdefault(key, []).append(benchmark[\"garbage_collector\"])\n",
    "    import pprint\n",
    "    for key in benchmarks:\n",
    "        if len(benchmarks[key].get(\"success\", {})) == 1:\n",
    "            print(key)\n",
    "find_only_success()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2993e20b-893a-4622-86c3-bdea1460ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_benchmarks_len_report():\n",
    "    benchmarks = {}\n",
    "    for r in [\"Graal\", \"HotSpot\"]:\n",
    "        for group in [\"DaCapo\", \"Renaissance\"]:\n",
    "            for file in Path(\"benchmark_stats/\").glob(f\"{group}*{r}*.json\"):\n",
    "                with open(file) as f: \n",
    "                    b = json.loads(f.read())\n",
    "                    gc = b[\"garbage_collector\"]\n",
    "                    name = b[\"benchmark_name\"]\n",
    "                    if name == \"spring\":\n",
    "                        print(\"Skipping\")\n",
    "                        continue\n",
    "                    status = \"error\" if bool(b[\"error\"]) else \"success\"\n",
    "                    hs = b[\"heap_size\"]\n",
    "                    entry = benchmarks.setdefault(r, {}).setdefault(hs, {}).setdefault(gc, {}).setdefault(status, [])\n",
    "                    entry.append(name)\n",
    "    nums = {}\n",
    "    for r, h_dict in benchmarks.items():\n",
    "        for heap, gc_dict in h_dict.items():\n",
    "            for gc in gc_dict:\n",
    "                error = gc_dict[gc].get(\"error\", [])\n",
    "                success = gc_dict[gc].get(\"success\", [])\n",
    "                error = len(error)\n",
    "                success = len(success)\n",
    "                gc_dict[gc][\"error\"] = error\n",
    "                gc_dict[gc][\"success\"] = success\n",
    "                nums.setdefault(heap,[]).append(error + success)\n",
    "                \n",
    "    for key in nums:\n",
    "        print(f\"{nums[key]=}\")\n",
    "        assert len(set(nums[key])) == 1\n",
    "    \n",
    "    with open(\"benchmarks_len_report.json\", \"w\") as f:\n",
    "        f.write(json.dumps(benchmarks))\n",
    "    print(benchmarks)\n",
    "compute_benchmarks_len_report()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d19e22de-087d-4845-aff9-f3f70749e8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graal: Parallel: old=68.40239091999999 new=0.163538928\n",
      "Graal: Z: old=80.64084448 new=0.06809886720000001\n",
      "Graal: G1: old=70.56356804 new=0.25176090240000004\n",
      "HotSpot: Parallel: old=64.61255403 new=0.1650158783\n",
      "HotSpot: Z: old=75.97102196 new=0.0776383012\n",
      "HotSpot: G1: old=65.32373374999999 new=0.24388854999999995\n"
     ]
    }
   ],
   "source": [
    "def eq1(t_weight, t_score, p_weight, p_score):\n",
    "    return t_weight * t_score + p_weight * p_score\n",
    "    \n",
    "def eq2(t_weight, t_score, p_weight, p_score):\n",
    "    if t_weight == 0:\n",
    "        t_weight = t_score = 1\n",
    "    if p_weight == 0:\n",
    "        p_weight = p_score = 1\n",
    "    return t_weight * t_score * p_weight * p_score\n",
    "\n",
    "def all():\n",
    "    dir = Path(\"benchmark_matrix\")\n",
    "    with open(dir / \"Graal_11_10_0_19.json\") as f:\n",
    "        graal = json.loads(f.read())[\"matrix\"][\"512\"]\n",
    "    with open(dir / \"HotSpot_11_10_0_19.json\") as f:\n",
    "        hotspot = json.loads(f.read())[\"matrix\"][\"512\"]\n",
    "    \n",
    "    for (t,p) in zip([1,0],[0,1]):\n",
    "        for gc in graal:\n",
    "            gc_t = graal[gc][\"throughput\"]\n",
    "            gc_p = graal[gc][\"pause_time\"]\n",
    "            #print(gc_t, gc_p)\n",
    "            score_1 = eq1(t, gc_t, p, gc_p)\n",
    "            score_2 = eq2(t, gc_t, p, gc_p)\n",
    "            print(f\"Graal: {gc}, {t=}, {p=} {score_1=} {score_2=}\")\n",
    "    for (t,p) in zip([1,0],[0,1]):\n",
    "        for gc in hotspot:\n",
    "            gc_t = graal[gc][\"throughput\"]\n",
    "            gc_p = graal[gc][\"pause_time\"]\n",
    "            score_1 = eq1(t, hotspot[gc][\"throughput\"], p, hotspot[gc][\"pause_time\"])\n",
    "            score_2 = eq2(t, hotspot[gc][\"throughput\"], p, hotspot[gc][\"pause_time\"])\n",
    "            print(f\"HotSpot: {gc}, {t=}, {p=} {score_1=} {score_2=}\")\n",
    "\n",
    "\n",
    "t_weight = 0.72\n",
    "p_weight = 0.28\n",
    "old = eq1(t_weight, 95, p_weight, 0.008539)\n",
    "new = eq2(t_weight, 95, p_weight, 0.008539)\n",
    "print(f\"Graal: Parallel: {old=} {new=}\")\n",
    "old = eq1(t_weight, 112, p_weight, 0.003016)\n",
    "new = eq2(t_weight, 112, p_weight, 0.003016)\n",
    "print(f\"Graal: Z: {old=} {new=}\")\n",
    "old = eq1(t_weight, 98, p_weight, 0.012743)\n",
    "new = eq2(t_weight, 98, p_weight, 0.012743)\n",
    "print(f\"Graal: G1: {old=} {new=}\")\n",
    "\n",
    "\n",
    "t_weight = 0.71\n",
    "p_weight = 0.29\n",
    "old = eq1(t_weight, 91, p_weight, 0.008807)\n",
    "new = eq2(t_weight, 91, p_weight, 0.008807)\n",
    "print(f\"HotSpot: Parallel: {old=} {new=}\")\n",
    "old = eq1(t_weight, 107, p_weight, 0.003524)\n",
    "new = eq2(t_weight, 107, p_weight, 0.003524)\n",
    "print(f\"HotSpot: Z: {old=} {new=}\")\n",
    "old = eq1(t_weight, 92, p_weight, 0.012875)\n",
    "new = eq2(t_weight, 92, p_weight, 0.012875)\n",
    "print(f\"HotSpot: G1: {old=} {new=}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6502ded-fff8-448f-8517-1b7aa4b65382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eq1(t_weight, t_score, p_weight, p_score):\n",
    "    return t_weight * t_score + p_weight * p_score\n",
    "    \n",
    "def eq2(t_weight, t_score, p_weight, p_score):\n",
    "    if t_weight == 0:\n",
    "        t_weight = t_score = 1\n",
    "    if p_weight == 0:\n",
    "        p_weight = p_score = 1\n",
    "    return t_weight * t_score * p_weight * p_score\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
